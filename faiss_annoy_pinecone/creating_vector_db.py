from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv
import time
import json
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import hashlib
import re
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
        text = re.sub(r'[^\w\sê°€-í£.,!?;:]', ' ', text)
        return text.strip()
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk = ' '.join(words[i:i + self.chunk_size])
            chunks.append(chunk)
            
        return chunks
    
    def process_documents(self, documents: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """ë¬¸ì„œë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• """
        processed_chunks = []
        
        for doc in documents:
            content = self.clean_text(doc['content'])
            chunks = self.split_text_into_chunks(content)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc.get('source', 'unknown')}_{i}"
                processed_chunks.append({
                    'id': chunk_id,
                    'content': chunk,
                    'source': doc.get('source', 'unknown'),
                    'metadata': doc.get('metadata', {}),
                    'chunk_index': i,
                    'timestamp': datetime.now().isoformat()
                })
        
        return processed_chunks

class VectorSearchEngine:
    """ë²¡í„° ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.pc = None
        self.index = None
        self.index_name = None
        logger.info(f"âœ… SentenceTransformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    
    def init_pinecone(self) -> None:
        """Pinecone ì´ˆê¸°í™”"""
        load_dotenv()
        api_key = os.environ.get('PINECONE_API_KEY')
        if not api_key:
            raise ValueError("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.pc = Pinecone(api_key=api_key)
        logger.info("âœ… Pinecone ì´ˆê¸°í™” ì„±ê³µ")
    
    def create_index(self, index_name: str, dimension: int, cloud: str = "aws", region: str = "us-east-1") -> None:
        """ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°"""
        if not self.pc:
            self.init_pinecone()
        
        existing_indexes = [idx['name'] for idx in self.pc.list_indexes()]
        
        if index_name in existing_indexes:
            logger.info(f"âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì—°ê²°: {index_name}")
        else:
            self.pc.create_index(
                name=index_name,
                dimension=dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud=cloud, region=region)
            )
            logger.info(f"âœ… ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index_name}")
            
            # ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            self._wait_for_index_ready(index_name)
        
        self.index = self.pc.Index(index_name)
        self.index_name = index_name
        logger.info(f"âœ… ì¸ë±ìŠ¤ ì—°ê²° ì™„ë£Œ: {index_name}")
    
    def _wait_for_index_ready(self, index_name: str, max_wait: int = 300) -> None:
        """ì¸ë±ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°"""
        start_time = time.time()
        while time.time() - start_time < max_wait:
            try:
                index = self.pc.Index(index_name)
                # ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
                stats = index.describe_index_stats()
                logger.info(f"â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘... (ê²½ê³¼ ì‹œê°„: {int(time.time() - start_time)}ì´ˆ)")
                logger.info(f"ğŸ“Š ì¸ë±ìŠ¤ ìƒíƒœ: {stats}")
                time.sleep(5)
                return  # ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë¨
            except Exception as e:
                logger.info(f"â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ëŒ€ê¸° ì¤‘... ì—ëŸ¬: {e}")
                time.sleep(5)
        else:
            raise TimeoutError(f"ì¸ë±ìŠ¤ {index_name}ì´ {max_wait}ì´ˆ ë‚´ì— ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = self.model.encode(texts, show_progress_bar=True)
        logger.info(f"âœ… {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ")
        return embeddings.tolist()
    
    def insert_documents(self, documents: List[Dict[str, str]], batch_size: int = 100) -> None:
        """ë¬¸ì„œë“¤ì„ ë²¡í„° DBì— ì‚½ì…"""
        if not self.index:
            raise ValueError("ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. create_index()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        logger.info(f"ğŸ”„ {len(documents)}ê°œ ë¬¸ì„œì˜ ì„ë² ë”© ìƒì„± ì‹œì‘")
        texts = [doc['content'] for doc in documents]
        embeddings = self.embed_texts(texts)
        
        logger.info(f"ğŸ“ ìƒì„±ëœ ì„ë² ë”© ì •ë³´: ê°œìˆ˜={len(embeddings)}, ì°¨ì›={len(embeddings[0]) if embeddings else 0}")
        
        # ë°°ì¹˜ë¡œ ì²˜ë¦¬
        total_inserted = 0
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            
            vectors = []
            for j, (doc, embedding) in enumerate(zip(batch_docs, batch_embeddings)):
                vector_id = doc.get('id', f"doc_{i+j}")
                metadata = {
                    'content': doc['content'][:1000],  # Pinecone ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ
                    'source': doc.get('source', 'unknown'),
                    'chunk_index': doc.get('chunk_index', 0),
                    'timestamp': doc.get('timestamp', datetime.now().isoformat())
                }
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í¬í•¨
                if 'metadata' in doc:
                    for key, value in doc['metadata'].items():
                        if isinstance(value, (str, int, float, bool)):  # Pineconeì´ ì§€ì›í•˜ëŠ” íƒ€ì…ë§Œ
                            metadata[key] = value
                
                vectors.append((vector_id, embedding, metadata))
            
            logger.info(f"ğŸ’¾ ë°°ì¹˜ {i//batch_size + 1}: {len(vectors)}ê°œ ë²¡í„° ì‚½ì… ì¤‘...")
            
            try:
                upsert_response = self.index.upsert(vectors=vectors)
                logger.info(f"âœ… ë°°ì¹˜ {i//batch_size + 1} ì‚½ì… ì‘ë‹µ: {upsert_response}")
                total_inserted += len(vectors)
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì‚½ì… ì‹¤íŒ¨: {e}")
                raise
            
            # ì‚½ì… í›„ ì ì‹œ ëŒ€ê¸° (Pinecone ì¸ë±ì‹± ì‹œê°„ í™•ë³´)
            time.sleep(2)
        
        # ì¸ë±ì‹± ì™„ë£Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
        logger.info("â³ ë²¡í„° ì¸ë±ì‹± ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        for wait_time in [5, 10, 15]:  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸°
            time.sleep(wait_time)
            stats = self.index.describe_index_stats()
            current_count = stats.get('total_vector_count', 0)
            logger.info(f"ğŸ“Š ëŒ€ê¸° ì¤‘... í˜„ì¬ ë²¡í„° ìˆ˜: {current_count}")
            if current_count > 0:
                break
        
        # ìµœì¢… í™•ì¸
        final_stats = self.index.describe_index_stats()
        actual_count = final_stats.get('total_vector_count', 0)
        logger.info(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì‚½ì… ìš”ì²­ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì‹¤ì œ ì¸ë±ìŠ¤ì— ì €ì¥ëœ ë²¡í„° ìˆ˜: {actual_count}")
        
        if actual_count == 0:
            logger.warning("âš ï¸ ë²¡í„°ê°€ ì¸ë±ìŠ¤ì— ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Pinecone ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            # ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
            logger.info(f"ì¸ë±ìŠ¤ í†µê³„: {final_stats}")
        
        return actual_count
    
    def search(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict]:
        """ì¿¼ë¦¬ë¥¼ í†µí•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.index:
            raise ValueError("ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        query_embedding = self.model.encode([query])[0].tolist()
        
        result = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        matches = []
        logger.info(f"ğŸ” ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼: {len(result.get('matches', []))}ê°œ")
        
        for i, match in enumerate(result.get('matches', [])):
            score = match['score']
            logger.info(f"  - ë§¤ì¹˜ {i+1}: ID={match['id']}, ì ìˆ˜={score:.4f}")
            
            if score >= score_threshold:
                matches.append({
                    'id': match['id'],
                    'score': score,
                    'content': match['metadata'].get('content', ''),
                    'source': match['metadata'].get('source', ''),
                    'metadata': match['metadata']
                })
        
        logger.info(f"ğŸ” ì¿¼ë¦¬ '{query}'ì— ëŒ€í•´ {len(matches)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬ (ì„ê³„ê°’: {score_threshold})")
        return matches
    
    def delete_index(self) -> None:
        """ì¸ë±ìŠ¤ ì‚­ì œ"""
        if self.pc and self.index_name:
            self.pc.delete_index(self.index_name)
            logger.info(f"ğŸ—‘ï¸ ì¸ë±ìŠ¤ {self.index_name} ì‚­ì œ ì™„ë£Œ")
    
    def get_index_stats(self) -> Dict:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        if not self.index:
            raise ValueError("ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        stats = self.index.describe_index_stats()
        return stats

class RAGSystem:
    """RAG ì‹œìŠ¤í…œ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', chunk_size: int = 500):
        self.document_processor = DocumentProcessor(chunk_size=chunk_size)
        self.search_engine = VectorSearchEngine(model_name=model_name)
        self.search_engine.init_pinecone()
    
    def load_documents_from_file(self, file_path: str) -> List[Dict[str, str]]:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        file_path = Path(file_path)
        
        if file_path.suffix == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append({
                    'content': content,
                    'source': str(file_path),
                    'metadata': {'file_type': 'txt'}
                })
        elif file_path.suffix == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    documents.extend(data)
                else:
                    documents.append(data)
        
        return documents
    
    def setup_knowledge_base(self, documents: List[Dict[str, str]], index_name: str = None) -> str:
        """ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì •"""
        if not index_name:
            timestamp = int(time.time())
            index_name = f"rag-knowledge-base-{timestamp}"
        
        logger.info(f"ğŸ“„ ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # ë¬¸ì„œ ì „ì²˜ë¦¬
        processed_docs = self.document_processor.process_documents(documents)
        logger.info(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(processed_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ì²˜ë¦¬ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸
        for i, doc in enumerate(processed_docs[:3]):  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
            logger.info(f"ì²­í¬ {i}: ID={doc['id']}, ê¸¸ì´={len(doc['content'])}, ë‚´ìš©={doc['content'][:100]}...")
        
        # ì„ë² ë”© ì°¨ì› ê³„ì‚°
        sample_embedding = self.search_engine.model.encode(["sample text"])
        dimension = sample_embedding.shape[1]
        logger.info(f"ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
        
        # ì¸ë±ìŠ¤ ìƒì„± ë° ë¬¸ì„œ ì‚½ì…
        self.search_engine.create_index(index_name, dimension)
        
        # ì‚½ì… ì „ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        initial_stats = self.search_engine.get_index_stats()
        logger.info(f"ğŸ“Š ì‚½ì… ì „ ë²¡í„° ìˆ˜: {initial_stats.get('total_vector_count', 0)}")
        
        self.search_engine.insert_documents(processed_docs)
        
        # ì‚½ì… í›„ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        final_stats = self.search_engine.get_index_stats()
        logger.info(f"ğŸ“Š ì‚½ì… í›„ ë²¡í„° ìˆ˜: {final_stats.get('total_vector_count', 0)}")
        
        return index_name
    
    def query(self, question: str, top_k: int = 3, score_threshold: float = 0.0) -> Dict:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        matches = self.search_engine.search(question, top_k, score_threshold)
        
        context = "\n\n".join([match['content'] for match in matches])
        
        return {
            'question': question,
            'context': context,
            'sources': matches,
            'timestamp': datetime.now().isoformat()
        }
    
    def get_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        return self.search_engine.get_index_stats()

class AdvancedRAGSystem(RAGSystem):
    """ê³ ê¸‰ RAG ì‹œìŠ¤í…œ - ë‹µë³€ ìƒì„± ë° ì¶”ê°€ ê¸°ëŠ¥ í¬í•¨"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', chunk_size: int = 500):
        super().__init__(model_name, chunk_size)
        self.conversation_history = []
    
    def generate_answer(self, question: str, top_k: int = 3, score_threshold: float = 0.2) -> Dict:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        search_result = self.query(question, top_k, score_threshold)
        
        if not search_result['sources']:
            return {
                'question': question,
                'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'confidence': 0.0,
                'sources': [],
                'timestamp': datetime.now().isoformat()
            }
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì†ŒìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        best_source = search_result['sources'][0]
        confidence = best_source['score']
        
        # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ë¡œì§ (ì‹¤ì œë¡œëŠ” LLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
        answer = self._generate_contextual_answer(question, search_result['context'])
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        self.conversation_history.append({
            'question': question,
            'answer': answer,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        })
        
        return {
            'question': question,
            'answer': answer,
            'confidence': confidence,
            'sources': search_result['sources'],
            'context': search_result['context'],
            'timestamp': datetime.now().isoformat()
        }
    
    def _generate_contextual_answer(self, question: str, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ê°„ë‹¨í•œ ì¶”ì¶œ ë°©ì‹)"""
        context_sentences = context.split('.')
        
        # ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì¥ë“¤ ì°¾ê¸°
        question_keywords = set(question.lower().replace('?', '').split())
        
        relevant_sentences = []
        for sentence in context_sentences:
            sentence_words = set(sentence.lower().split())
            overlap = len(question_keywords & sentence_words)
            if overlap > 0:
                relevant_sentences.append((sentence.strip(), overlap))
        
        if relevant_sentences:
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì¥ë“¤ ì •ë ¬
            relevant_sentences.sort(key=lambda x: x[1], reverse=True)
            answer = '. '.join([s[0] for s in relevant_sentences[:2]])
            return answer + '.' if answer else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ì§€ë§Œ êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            return "ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def ask_followup(self, question: str) -> Dict:
        """ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ í›„ì† ì§ˆë¬¸ ì²˜ë¦¬"""
        if self.conversation_history:
            # ì´ì „ ì§ˆë¬¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í˜„ì¬ ì§ˆë¬¸ì— í†µí•©
            recent_context = self.conversation_history[-1]['answer']
            enhanced_question = f"{question} (ì´ì „ ì»¨í…ìŠ¤íŠ¸: {recent_context[:100]}...)"
            return self.generate_answer(enhanced_question)
        else:
            return self.generate_answer(question)
    
    def get_conversation_summary(self) -> Dict:
        """ëŒ€í™” ìš”ì•½ ì •ë³´"""
        if not self.conversation_history:
            return {"message": "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        return {
            'total_questions': len(self.conversation_history),
            'average_confidence': sum(h['confidence'] for h in self.conversation_history) / len(self.conversation_history),
            'recent_questions': [h['question'] for h in self.conversation_history[-3:]],
            'start_time': self.conversation_history[0]['timestamp'],
            'last_time': self.conversation_history[-1]['timestamp']
        }
    
    def suggest_related_questions(self, current_question: str) -> List[str]:
        """í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±"""
        search_result = self.query(current_question, top_k=5, score_threshold=0.1)
        
        suggestions = []
        if search_result['sources']:
            for source in search_result['sources'][:3]:
                content = source['content']
                source_name = source['source']
                
                # ì†ŒìŠ¤ë³„ ê´€ë ¨ ì§ˆë¬¸ ì œì•ˆ
                if 'pinecone' in source_name.lower():
                    suggestions.extend([
                        "Pineconeì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
                    ])
                elif 'rag' in source_name.lower():
                    suggestions.extend([
                        "RAG ì‹œìŠ¤í…œì˜ êµ¬ì„±ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                        "RAGì˜ í•œê³„ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                    ])
                elif 'embedding' in content.lower() or 'ml_fundamentals' in source_name:
                    suggestions.extend([
                        "ì„ë² ë”© ëª¨ë¸ì€ ì–´ë–»ê²Œ ì„ íƒí•˜ë‚˜ìš”?",
                        "BERTì™€ RoBERTaì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                    ])
                elif 'chunk' in content.lower() or 'preprocessing' in source_name:
                    suggestions.extend([
                        "ìµœì ì˜ ì²­í¬ í¬ê¸°ëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
                        "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë°©ë²•ì—ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?"
                    ])
        
        # ì¤‘ë³µ ì œê±° ë° í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ê²ƒë§Œ ë°˜í™˜
        unique_suggestions = list(set(suggestions))
        return [s for s in unique_suggestions if s.lower() != current_question.lower()][:5]
    
    def load_documents_from_file(self, file_path: str) -> List[Dict[str, str]]:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        file_path = Path(file_path)
        
        if file_path.suffix == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append({
                    'content': content,
                    'source': str(file_path),
                    'metadata': {'file_type': 'txt'}
                })
        elif file_path.suffix == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    documents.extend(data)
                else:
                    documents.append(data)
        
        return documents
    
    def setup_knowledge_base(self, documents: List[Dict[str, str]], index_name: str = None) -> str:
        """ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì •"""
        if not index_name:
            timestamp = int(time.time())
            index_name = f"rag-knowledge-base-{timestamp}"
        
        logger.info(f"ğŸ“„ ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # ë¬¸ì„œ ì „ì²˜ë¦¬
        processed_docs = self.document_processor.process_documents(documents)
        logger.info(f"ğŸ“„ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {len(processed_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ì²˜ë¦¬ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸
        for i, doc in enumerate(processed_docs[:3]):  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
            logger.info(f"ì²­í¬ {i}: ID={doc['id']}, ê¸¸ì´={len(doc['content'])}, ë‚´ìš©={doc['content'][:100]}...")
        
        # ì„ë² ë”© ì°¨ì› ê³„ì‚°
        sample_embedding = self.search_engine.model.encode(["sample text"])
        dimension = sample_embedding.shape[1]
        logger.info(f"ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
        
        # ì¸ë±ìŠ¤ ìƒì„± ë° ë¬¸ì„œ ì‚½ì…
        self.search_engine.create_index(index_name, dimension)
        
        # ì‚½ì… ì „ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        initial_stats = self.search_engine.get_index_stats()
        logger.info(f"ğŸ“Š ì‚½ì… ì „ ë²¡í„° ìˆ˜: {initial_stats.get('total_vector_count', 0)}")
        
        self.search_engine.insert_documents(processed_docs)
        
        # ì‚½ì… í›„ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        final_stats = self.search_engine.get_index_stats()
        logger.info(f"ğŸ“Š ì‚½ì… í›„ ë²¡í„° ìˆ˜: {final_stats.get('total_vector_count', 0)}")
        
        return index_name
    
    def query(self, question: str, top_k: int = 3, score_threshold: float = 0.0) -> Dict:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        matches = self.search_engine.search(question, top_k, score_threshold)
        
        context = "\n\n".join([match['content'] for match in matches])
        
        return {
            'question': question,
            'context': context,
            'sources': matches,
            'timestamp': datetime.now().isoformat()
        }
    
    def get_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        return self.search_engine.get_index_stats()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag = RAGSystem(chunk_size=300)
        
        # ìƒ˜í”Œ ë¬¸ì„œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ APIì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ)
        sample_documents = [
            {
                'content': """
                Pineconeì€ ë¨¸ì‹ ëŸ¬ë‹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. 
                ê³ ì°¨ì› ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆìœ¼ë©°, 
                ì‹¤ì‹œê°„ ìœ ì‚¬ë„ ê²€ìƒ‰ê³¼ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
                Serverless ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•˜ì—¬ í™•ì¥ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.
                """,
                'source': 'pinecone_docs',
                'metadata': {'category': 'database', 'language': 'korean'}
            },
            {
                'content': """
                ë²¡í„° ì„ë² ë”©ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“±ì˜ ë°ì´í„°ë¥¼ ê³ ì°¨ì› ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
                Sentence TransformersëŠ” ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ,
                BERT, RoBERTa ë“±ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
                ì´ë¥¼ í†µí•´ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì„ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """,
                'source': 'ml_fundamentals',
                'metadata': {'category': 'machine_learning', 'language': 'korean'}
            },
            {
                'content': """
                RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬
                ì–¸ì–´ ëª¨ë¸ì˜ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.
                ì´ ë°©ë²•ì€ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ,
                ìµœì‹  ì •ë³´ë‚˜ ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
                ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
                """,
                'source': 'rag_guide',
                'metadata': {'category': 'ai_techniques', 'language': 'korean'}
            },
            {
                'content': """
                ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì²­í‚¹(chunking)ì€ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
                íš¨ê³¼ì ì¸ ì²­í‚¹ì€ ë¬¸ë§¥ì„ ë³´ì¡´í•˜ë©´ì„œë„ ê²€ìƒ‰ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
                ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ ê²½ê³„, ë‹¨ë½, ë˜ëŠ” ì˜ë¯¸ì  ë‹¨ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
                ì²­í¬ í¬ê¸°ì™€ ì˜¤ë²„ë©ì„ ì ì ˆíˆ ì¡°ì •í•˜ì—¬ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•´ì•¼ í•©ë‹ˆë‹¤.
                """,
                'source': 'nlp_preprocessing',
                'metadata': {'category': 'preprocessing', 'language': 'korean'}
            }
        ]
        
        # ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì •
        logger.info("ğŸš€ ì§€ì‹ ë² ì´ìŠ¤ ì„¤ì • ì‹œì‘")
        index_name = rag.setup_knowledge_base(sample_documents)
        
        # ì„¤ì • ì™„ë£Œ í›„ ìƒíƒœ í™•ì¸
        final_stats = rag.get_stats()
        logger.info(f"ğŸ“Š ì„¤ì • ì™„ë£Œ í›„ ë²¡í„° ìˆ˜: {final_stats.get('total_vector_count', 0)}")
        
        if final_stats.get('total_vector_count', 0) == 0:
            logger.error("âŒ ë²¡í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        test_queries = [
            "Pineconeì´ ë¬´ì—‡ì¸ê°€ìš”?",
            "RAG ê¸°ë²•ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ë²¡í„° ì„ë² ë”©ì˜ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì²­í‚¹ì€ ì™œ ì¤‘ìš”í•œê°€ìš”?"
        ]
        
        print("\n" + "="*80)
        print("ğŸ” RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*80)
        
        for query in test_queries:
            print(f"\nì§ˆë¬¸: {query}")
            print("-" * 50)
            
            result = rag.query(query, top_k=3, score_threshold=0.0)  # ì„ê³„ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
            
            if result['sources']:
                for i, source in enumerate(result['sources'], 1):
                    print(f"{i}. [ì ìˆ˜: {source['score']:.3f}] {source['source']}")
                    print(f"   ë‚´ìš©: {source['content'][:150]}...")
                    print()
            else:
                print("âŒ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ë””ë²„ê·¸ë¥¼ ìœ„í•´ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
                stats = rag.get_stats()
                print(f"   í˜„ì¬ ì¸ë±ìŠ¤ì˜ ë²¡í„° ìˆ˜: {stats.get('total_vector_count', 0)}")
                
                # ë” ìì„¸í•œ ë””ë²„ê·¸ ì •ë³´
                if stats.get('total_vector_count', 0) > 0:
                    print("   ë²¡í„°ëŠ” ì¡´ì¬í•˜ì§€ë§Œ ê²€ìƒ‰ë˜ì§€ ì•ŠìŒ - ì„ë² ë”© ë˜ëŠ” ì¿¼ë¦¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š ì‹œìŠ¤í…œ í†µê³„")
        print("="*80)
        stats = rag.get_stats()
        print(f"ì´ ë²¡í„° ìˆ˜: {stats.get('total_vector_count', 0)}")
        print(f"ì¸ë±ìŠ¤ ì´ë¦„: {index_name}")
        
        # ì •ë¦¬ ì˜µì…˜ (ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬)
        cleanup = input("\nì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
        if cleanup == 'y':
            rag.search_engine.delete_index()
        
    except Exception as e:
        logger.error(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()