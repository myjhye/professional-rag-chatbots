import os
import sys
import logging
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass
import json
from datetime import datetime

# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ìž„í¬íŠ¸
from langchain_community.document_loaders import CSVLoader # CSV íŒŒì¼ ë¡œë“œìš©
from langchain.text_splitter import CharacterTextSplitter # í…ìŠ¤íŠ¸ ì²­í‚¹ìš© (í˜„ìž¬ ë¯¸ì‚¬ìš©)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI # OpenAI API ì¸í„°íŽ˜ì´ìŠ¤
from langchain_community.vectorstores import FAISS # ë¡œì»¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
from langchain.chains import RetrievalQA # ê²€ìƒ‰+ë‹µë³€ ìƒì„± ì²´ì¸
from langchain.prompts import PromptTemplate # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
from langchain.schema import Document # ë¬¸ì„œ ê°ì²´ í´ëž˜ìŠ¤

# ìœ í‹¸ë¦¬í‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import tiktoken # OpenAI í† í° ê³„ì‚°ìš©
from tenacity import retry, stop_after_attempt, wait_exponential # ìž¬ì‹œë„ ë¡œì§ìš© (í˜„ìž¬ ë¯¸ì‚¬ìš©)
import warnings
warnings.filterwarnings("ignore", category=UserWarning) # ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¹€


"""
    RAG ì‹œìŠ¤í…œ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ëž˜ìŠ¤

    ëª¨ë“  ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ì•™ ì§‘ì¤‘ì‹ìœ¼ë¡œ ê´€ë¦¬
"""
@dataclass
class Config:
    # ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
    chunk_size: int = 1000 # í…ìŠ¤íŠ¸ ì²­í‚¹ í¬ê¸° (í˜„ìž¬ CSVëŠ” í–‰ë³„ ì²˜ë¦¬ë¡œ ë¯¸ì‚¬ìš©)
    chunk_overlap: int = 100 # ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸° (í˜„ìž¬ ë¯¸ì‚¬ìš©)
    
    # LLM ê´€ë ¨ ì„¤ì •
    max_tokens: int = 150 # LLM ì‘ë‹µ ìµœëŒ€ í† í° ìˆ˜ (ë¹„ìš© ì œì–´)
    model_name: str = "gpt-4o" # ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
    temperature: float = 0.4 # ì°½ì˜ì„± vs ì¼ê´€ì„± ê· í˜• (0=ì¼ê´€ì„±, 1=ì°½ì˜ì„±)
    
    # ê²€ìƒ‰ ê´€ë ¨ ì„¤ì •
    retrieval_k: int = 4 # ê²€ìƒ‰í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜
    embedding_model: str = "text-embedding-ada-002" # ìž„ë² ë”© ëª¨ë¸ëª…
    
    # API ê´€ë ¨ ì„¤ì •
    max_retries: int = 3 # API í˜¸ì¶œ ìž¬ì‹œë„ íšŸìˆ˜
    request_timeout: int = 30 # API ìš”ì²­ íƒ€ìž„ì•„ì›ƒ (ì´ˆ)

    @classmethod
    def from_file(cls, config_path: str) -> 'Config':
        try:
            with open(config_path, 'r') as f:
                config_dict = json.load(f)
            # ë”•ì…”ë„ˆë¦¬ë¥¼ Config ê°ì²´ë¡œ ë³€í™˜
            return cls(**config_dict)
        except FileNotFoundError:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            logging.warning(f"ì„¤ì • íŒŒì¼ {config_path}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return cls()
        except json.JSONDecodeError as e:
            # JSON í˜•ì‹ì´ ìž˜ëª»ë˜ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            logging.error(f"ì„¤ì • íŒŒì¼ì˜ JSON í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
            return cls()




"""
    CSV ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ RAG (Retrieval Augmented Generation) íŒŒì´í”„ë¼ì¸

    ì£¼ìš” ê¸°ëŠ¥:
    1. CSV ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    2. OpenAI ìž„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”
    3. FAISS ë²¡í„°ìŠ¤í† ì–´ì— ì €ìž¥
    4. ìžì—°ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    5. ëŒ€í™” ê¸°ë¡ ë° ë¹„ìš© ì¶”ì 
"""
class CSVRAGPipeline:   
    def __init__(self, config: Config):
        # ì„¤ì • ì €ìž¥
        self.config = config

        # í•µì‹¬ êµ¬ì„±ìš”ì†Œë“¤ ì´ˆê¸°í™” (ë‚˜ì¤‘ì— ì„¤ì •ë¨)
        self.vectorstore: Optional[FAISS] = None # FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
        self.qa_chain: Optional[RetrievalQA] = None # ì§ˆì˜ì‘ë‹µ ì²´ì¸
        self.embeddings: Optional[OpenAIEmbeddings] = None # ìž„ë² ë”© ëª¨ë¸
        self.documents: List[Document] = [] # ë¡œë“œëœ ë¬¸ì„œë“¤

        # ë¹„ìš© ì¶”ì ìš© í† í° ì¹´ìš´í„°
        self.encoding = tiktoken.encoding_for_model(self.config.model_name) # í† í° ì¸ì½”ë”
        self.total_tokens_used = 0 # ëˆ„ì  í† í° ì‚¬ìš©ëŸ‰

        logging.info("CSV RAG íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

    def load_csv_data(self, csv_path: str) -> List[Document]:
        df = pd.read_csv(csv_path)

        documents = []
        for _, row in df.iterrows():
            # ëª¨ë“  ì»¬ëŸ¼ì„ í¬í•¨í•˜ì—¬ content ìƒì„±
            content = f"""Model: {row['model']}
                            Color: {row['color']}
                            Fuel Type: {row['fuel_type']}
                            Transmission: {row['transmission']}
                            Price: ${row['price']:,.2f}
                            Manufacture Date: {row['manufacture_date']}
                            Sale Date: {row['sale_date']}
                            State: {row['state']}
                            Mileage: {row['mileage']:,.1f} miles"""
            
            metadata = {
                "source": "tesla_motors_data",
                "model": row['model'],
                "color": row['color'],
                "state": row['state'],
                "price": row['price'],
                "mileage": row['mileage']
            }
            documents.append(Document(page_content=content, metadata=metadata))

        self.documents = documents  # ë¬¸ì„œ ì €ìž¥
        logging.info(f"CSV ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
        return documents
    
    def create_embeddings(self, documents: List[Document]) -> Tuple[List[str], List[List[float]]]:
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)

        texts = [doc.page_content for doc in documents]
        embeddings = self.embeddings.embed_documents(texts)
        logging.info(f"ìž„ë² ë”© {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
        return texts, embeddings
    
    def create_vectorstore(self, documents: List[Document]) -> FAISS:
        """ë¬¸ì„œë¡œë¶€í„° ì§ì ‘ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
        
        # FAISS.from_documentsë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ìƒì„±
        vectorstore = FAISS.from_documents(documents, self.embeddings)
        logging.info("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vectorstore
    
    def setup_qa_chain(self, vectorstore: FAISS) -> None:
        self.vectorstore = vectorstore
        retriever = vectorstore.as_retriever(search_kwargs={"k": self.config.retrieval_k})
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
        prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. 
        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ê³ , ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸:
        {context}

        ì§ˆë¬¸: {question}

        ë‹µë³€:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(
                model=self.config.model_name, 
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            ),
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
        logging.info("QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")

    def process_query(self, query: str) -> Tuple[str, List[Document], Dict[str, Any]]:
        if self.qa_chain is None:
            raise RuntimeError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        start_time = time.time()
        result = self.qa_chain({"query": query})
        end_time = time.time()

        answer = result["result"]
        source_docs = result.get("source_documents", [])
        total_tokens = len(self.encoding.encode(query + answer))
        self.total_tokens_used += total_tokens

        metadata = {
            "processing_time": end_time - start_time,
            "total_tokens": total_tokens,
            "sources_count": len(source_docs)
        }

        return answer, source_docs, metadata

    def save_conversation_history(self, query: str, answer: str, sources: List[Document], metadata: Dict[str, Any]) -> None:
        """ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ìž¥"""
        conversation_log = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "answer": answer,
            "metadata": metadata,
            "sources": [{"content": doc.page_content, "metadata": doc.metadata} for doc in sources]
        }
        
        # ë¡œê·¸ íŒŒì¼ì— ì¶”ê°€
        log_file = "conversation_history.json"
        if os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        else:
            history = []
        
        history.append(conversation_log)
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def get_system_stats(self) -> Dict[str, Any]:
        return {
            "total_documents": len(self.documents),
            "total_tokens_used": self.total_tokens_used,
            "estimated_cost": self.total_tokens_used * 0.00001,  # ì˜ˆì‹œ: í† í°ë‹¹ ê°€ìƒì˜ ë¹„ìš©
            "vectorstore_loaded": self.vectorstore is not None,
            "qa_chain_ready": self.qa_chain is not None
        }

    def load_saved_vectorstore(self, vectorstore_path: str = "vectorstore") -> bool:
        """ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            if os.path.exists(vectorstore_path):
                self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
                self.vectorstore = FAISS.load_local(vectorstore_path, self.embeddings)
                logging.info("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logging.info("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logging.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def save_vectorstore(self, vectorstore_path: str = "vectorstore") -> bool:
        """ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥"""
        try:
            if self.vectorstore is not None:
                self.vectorstore.save_local(vectorstore_path)
                logging.info(f"ë²¡í„°ìŠ¤í† ì–´ë¥¼ {vectorstore_path}ì— ì €ìž¥í–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logging.warning("ì €ìž¥í•  ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logging.error(f"ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False


def create_sample_config() -> None:
    config = {
        "chunk_size": 1000,
        "chunk_overlap": 100,
        "max_tokens": 150,
        "model_name": "gpt-4o",
        "temperature": 0.4,
        "retrieval_k": 4,
        "embedding_model": "text-embedding-ada-002",
        "max_retries": 3,
        "request_timeout": 30
    }
    with open("config.json", 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print("ìƒ˜í”Œ ì„¤ì • íŒŒì¼ 'config.json'ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag_pipeline.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

    print("=== CSV RAG íŒŒì´í”„ë¼ì¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ===\n")

    try:
        config_path = input("ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ìž…ë ¥í•˜ì„¸ìš” (ì—”í„°: ê¸°ë³¸ ì„¤ì • ì‚¬ìš©): ").strip()
        if config_path and Path(config_path).exists():
            config = Config.from_file(config_path)
        else:
            config = Config()
            if not config_path:
                print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        pipeline = CSVRAGPipeline(config)

        use_saved = input("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
        if use_saved and pipeline.load_saved_vectorstore():
            print("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            pipeline.setup_qa_chain(pipeline.vectorstore)
        else:
            csv_path = "tesla_motors_data.csv"
            print("í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬:", os.getcwd())
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

            print("\nCSV ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì¤‘...")
            documents = pipeline.load_csv_data(csv_path)

            print("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            vectorstore = pipeline.create_vectorstore(documents)

            print("QA ì²´ì¸ ì„¤ì • ì¤‘...")
            pipeline.setup_qa_chain(vectorstore)

            # ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥ ì˜µì…˜
            save_vectorstore = input("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ìž¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            if save_vectorstore:
                pipeline.save_vectorstore()

        print("\nâœ… RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        print("ì´ì œ CSV ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
        print("ì‹œìŠ¤í…œ í†µê³„ë¥¼ ë³´ë ¤ë©´ 'stats'ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\n")

        # ëŒ€í™” ë£¨í”„
        while True:
            try:
                query = input("\nì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”: ").strip()

                if query.lower() in ['quit', 'ì¢…ë£Œ', 'exit']:
                    print("í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•ížˆ ê°€ì„¸ìš”!")
                    break

                if query.lower() == 'stats':
                    stats = pipeline.get_system_stats()
                    print("\n=== ì‹œìŠ¤í…œ í†µê³„ ===")
                    print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
                    print(f"ì‚¬ìš©ëœ ì´ í† í°: {stats['total_tokens_used']}")
                    print(f"ì¶”ì • ë¹„ìš©: ${stats['estimated_cost']:.4f}")
                    print(f"ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ: {'âœ… ë¡œë“œë¨' if stats['vectorstore_loaded'] else 'âŒ ë¯¸ë¡œë“œ'}")
                    print(f"QA ì²´ì¸ ìƒíƒœ: {'âœ… ì¤€ë¹„ë¨' if stats['qa_chain_ready'] else 'âŒ ë¯¸ì¤€ë¹„'}")
                    continue

                if not query:
                    print("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue

                answer, sources, metadata = pipeline.process_query(query)

                print(f"\nðŸ’¬ ë‹µë³€: {answer}")
                print(f"\nðŸ“Š ë©”íƒ€ë°ì´í„°:")
                print(f"  - ì²˜ë¦¬ ì‹œê°„: {metadata['processing_time']:.2f}ì´ˆ")
                print(f"  - ì‚¬ìš© í† í°: {metadata['total_tokens']}")
                print(f"  - ì°¸ì¡° ì†ŒìŠ¤: {metadata['sources_count']}ê°œ")

                if sources:
                    print(f"\nðŸ“š ì°¸ì¡° ì†ŒìŠ¤:")
                    for i, source in enumerate(sources[:3], 1):
                        content_preview = source.page_content[:150].replace('\n', ' ')
                        print(f"  {i}. {content_preview}...")

                pipeline.save_conversation_history(query, answer, sources, metadata)

            except KeyboardInterrupt:
                print("\n\ní”„ë¡œê·¸ëž¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                logging.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                continue

    except Exception as e:
        logging.error(f"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜: {e}")
    finally:
        try:
            if 'pipeline' in locals():
                stats = pipeline.get_system_stats()
                print(f"\n=== ìµœì¢… í†µê³„ ===")
                print(f"ì´ ì‚¬ìš© í† í°: {stats['total_tokens_used']}")
                print(f"ì´ ì¶”ì • ë¹„ìš©: ${stats['estimated_cost']:.4f}")
        except:
            pass


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--create-config":
        create_sample_config()
    else:
        main()