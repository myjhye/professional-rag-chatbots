import os
import sys
import logging
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import pandas as pd
from dataclasses import dataclass
import json
from datetime import datetime

# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ìž„í¬íŠ¸
from langchain_community.document_loaders import CSVLoader # CSV íŒŒì¼ ë¡œë“œìš©
from langchain.text_splitter import CharacterTextSplitter # í…ìŠ¤íŠ¸ ì²­í‚¹ìš© (í˜„ìž¬ ë¯¸ì‚¬ìš©)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI # OpenAI API ì¸í„°íŽ˜ì´ìŠ¤
from langchain_community.vectorstores import FAISS # ë¡œì»¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
from langchain.chains import RetrievalQA # ê²€ìƒ‰+ë‹µë³€ ìƒì„± ì²´ì¸
from langchain.prompts import PromptTemplate # LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
from langchain.schema import Document # ë¬¸ì„œ ê°ì²´ í´ëž˜ìŠ¤

# ìœ í‹¸ë¦¬í‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import tiktoken # OpenAI í† í° ê³„ì‚°ìš©
from tenacity import retry, stop_after_attempt, wait_exponential # ìž¬ì‹œë„ ë¡œì§ìš© (í˜„ìž¬ ë¯¸ì‚¬ìš©)
import warnings
warnings.filterwarnings("ignore", category=UserWarning) # ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¹€


"""
    RAG ì‹œìŠ¤í…œ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ëž˜ìŠ¤

    ëª¨ë“  ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ì•™ ì§‘ì¤‘ì‹ìœ¼ë¡œ ê´€ë¦¬
"""
@dataclass
class Config:
    # ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
    chunk_size: int = 1000 # í…ìŠ¤íŠ¸ ì²­í‚¹ í¬ê¸° (í˜„ìž¬ CSVëŠ” í–‰ë³„ ì²˜ë¦¬ë¡œ ë¯¸ì‚¬ìš©)
    chunk_overlap: int = 100 # ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸° (í˜„ìž¬ ë¯¸ì‚¬ìš©)
    
    # LLM ê´€ë ¨ ì„¤ì •
    max_tokens: int = 150 # LLM ì‘ë‹µ ìµœëŒ€ í† í° ìˆ˜ (ë¹„ìš© ì œì–´)
    model_name: str = "gpt-4o" # ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
    temperature: float = 0.4 # ì°½ì˜ì„± vs ì¼ê´€ì„± ê· í˜• (0=ì¼ê´€ì„±, 1=ì°½ì˜ì„±)
    
    # ê²€ìƒ‰ ê´€ë ¨ ì„¤ì •
    retrieval_k: int = 4 # ê²€ìƒ‰í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜
    embedding_model: str = "text-embedding-ada-002" # ìž„ë² ë”© ëª¨ë¸ëª…
    
    # API ê´€ë ¨ ì„¤ì •
    max_retries: int = 3 # API í˜¸ì¶œ ìž¬ì‹œë„ íšŸìˆ˜
    request_timeout: int = 30 # API ìš”ì²­ íƒ€ìž„ì•„ì›ƒ (ì´ˆ)

    @classmethod
    def from_file(cls, config_path: str) -> 'Config':
        try:
            with open(config_path, 'r') as f:
                config_dict = json.load(f)
            # ë”•ì…”ë„ˆë¦¬ë¥¼ Config ê°ì²´ë¡œ ë³€í™˜
            return cls(**config_dict)
        except FileNotFoundError:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            logging.warning(f"ì„¤ì • íŒŒì¼ {config_path}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return cls()
        except json.JSONDecodeError as e:
            # JSON í˜•ì‹ì´ ìž˜ëª»ë˜ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            logging.error(f"ì„¤ì • íŒŒì¼ì˜ JSON í˜•ì‹ì´ ìž˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
            return cls()




"""
    CSV ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ RAG (Retrieval Augmented Generation) íŒŒì´í”„ë¼ì¸

    ì£¼ìš” ê¸°ëŠ¥:
    1. CSV ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    2. OpenAI ìž„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”
    3. FAISS ë²¡í„°ìŠ¤í† ì–´ì— ì €ìž¥
    4. ìžì—°ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    5. ëŒ€í™” ê¸°ë¡ ë° ë¹„ìš© ì¶”ì 
"""
class CSVRAGPipeline:

    """
        RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    """   
    def __init__(self, config: Config):
        # ì„¤ì • ì €ìž¥
        self.config = config

        # í•µì‹¬ êµ¬ì„±ìš”ì†Œë“¤ ì´ˆê¸°í™” (ë‚˜ì¤‘ì— ì„¤ì •ë¨)
        self.vectorstore: Optional[FAISS] = None # FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
        self.qa_chain: Optional[RetrievalQA] = None # ì§ˆì˜ì‘ë‹µ ì²´ì¸
        self.embeddings: Optional[OpenAIEmbeddings] = None # ìž„ë² ë”© ëª¨ë¸
        self.documents: List[Document] = [] # ë¡œë“œëœ ë¬¸ì„œë“¤

        # ë¹„ìš© ì¶”ì ìš© í† í° ì¹´ìš´í„°
        self.encoding = tiktoken.encoding_for_model(self.config.model_name) # í† í° ì¸ì½”ë”
        self.total_tokens_used = 0 # ëˆ„ì  í† í° ì‚¬ìš©ëŸ‰

        logging.info("CSV RAG íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")


    """
        CSV íŒŒì¼ì„ ì½ì–´ì„œ LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        ê° CSV í–‰ì„ í•˜ë‚˜ì˜ Documentë¡œ ë³€í™˜í•˜ì—¬ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë§Œë“¤ê¸° (ë²¡í„° ê²€ìƒ‰ê³¼ LLM ì´í•´ë„ í–¥ìƒ)
    """
    def load_csv_data(self, csv_path: str) -> List[Document]:

        # pandasë¡œ CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(csv_path)

        documents = []

        # ê° í–‰ì„ Document ê°ì²´ë¡œ ë³€í™˜
        for _, row in df.iterrows():
            # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (ì´ë ‡ê²Œ í•˜ë©´ LLMì´ ì •ë³´ë¥¼ ë” ìž˜ ì´í•´)
            content = f"""Model: {row['model']}
                            Color: {row['color']}
                            Fuel Type: {row['fuel_type']}
                            Transmission: {row['transmission']}
                            Price: ${row['price']:,.2f}
                            Manufacture Date: {row['manufacture_date']}
                            Sale Date: {row['sale_date']}
                            State: {row['state']}
                            Mileage: {row['mileage']:,.1f} miles"""
            
            # ë©”íƒ€ë°ì´í„° ì„¤ì • (í•„í„°ë§ì´ë‚˜ í›„ì²˜ë¦¬ì— í™œìš©)
            metadata = {
                "source": "tesla_motors_data",
                "model": row['model'],
                "color": row['color'],
                "state": row['state'],
                "price": row['price'],
                "mileage": row['mileage']
            }

            # Document ê°ì²´ ìƒì„±
            documents.append(Document(page_content=content, metadata=metadata))

        # ë¬¸ì„œ ì €ìž¥ (ì‹œìŠ¤í…œ í†µê³„ìš©)
        self.documents = documents
        logging.info(f"CSV ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
        return documents
    

    """
        ë¬¸ì„œë“¤ì„ ìž„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” í˜„ìž¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        create_vectorstore()ì—ì„œ FAISS.from_documents()ë¥¼ ì‚¬ìš©í•˜ì—¬ 
        ë¬¸ì„œ ë¡œë“œì™€ ë²¡í„°í™”ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•¨
    """
    def create_embeddings(self, documents: List[Document]) -> Tuple[List[str], List[List[float]]]:
        # ìž„ë² ë”© ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)

        # ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        texts = [doc.page_content for doc in documents]

        # OpenAI APIë¥¼ í†µí•´ ìž„ë² ë”© ìƒì„± (ì´ ê³¼ì •ì—ì„œ API ë¹„ìš© ë°œìƒ)
        embeddings = self.embeddings.embed_documents(texts)
        
        logging.info(f"ìž„ë² ë”© {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
        return texts, embeddings
    

    """
        ë¬¸ì„œë“¤ë¡œë¶€í„° FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±
        
        ë‹¤ìŒ ê³¼ì • ìˆ˜í–‰:
        1. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        2. OpenAI APIë¡œ ìž„ë² ë”© ìƒì„±
        3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ìž¥
        4. ë©”íƒ€ë°ì´í„° ì—°ê²°
    """
    def create_vectorstore(self, documents: List[Document]) -> FAISS:
        # ìž„ë² ë”© ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
        
        # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        # ì´ í•œ ì¤„ì—ì„œ ë‹¤ìŒì´ ëª¨ë‘ ì¼ì–´ë‚œë‹¤:
        # 1. ê° ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ OpenAI APIë¡œ ë²¡í„°í™”
        # 2. FAISS ì¸ë±ìŠ¤ ìƒì„±
        # 3. ë²¡í„°ë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€
        # 4. ë©”íƒ€ë°ì´í„° ì—°ê²°
        vectorstore = FAISS.from_documents(documents, self.embeddings)
        
        logging.info("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vectorstore
    
    """
        ì§ˆì˜ì‘ë‹µ ì²´ì¸ ì„¤ì •
        
        ê²€ìƒ‰ê¸°(Retriever) + LLM + í”„ë¡¬í”„íŠ¸ë¥¼ ì—°ê²°í•˜ì—¬
        ì§ˆë¬¸ì— ëŒ€í•œ ìžë™ ë‹µë³€ ì‹œìŠ¤í…œì„ êµ¬ì¶•
    """
    def setup_qa_chain(self, vectorstore: FAISS) -> None:

        # ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥
        self.vectorstore = vectorstore

        # ê²€ìƒ‰ê¸° ì„¤ì •
        # search_kwargs={"k": 4}: ìƒìœ„ 4ê°œ ë¬¸ì„œë§Œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs={"k": self.config.retrieval_k})
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (ì´ í…œí”Œë¦¿ì´ LLMì˜ ë‹µë³€ í’ˆì§ˆì„ ì¢Œìš°)
        prompt_template = """
                ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. 
                ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ê³ , ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

                ì»¨í…ìŠ¤íŠ¸: {context}

                ì§ˆë¬¸: {question}

                ë‹µë³€:
            """
        
        # PromptTemplate ê°ì²´ ìƒì„±
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"] # í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
        )
        
        # QA ì²´ì¸ ìƒì„± (ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ì—°ê²°)
        self.qa_chain = RetrievalQA.from_chain_type(
            # LLM ì„¤ì •
            llm=ChatOpenAI(
                model=self.config.model_name, # ì‚¬ìš©í•  ëª¨ë¸ (ì˜ˆ: gpt-4o)
                temperature=self.config.temperature, # ì°½ì˜ì„± vs ì¼ê´€ì„±
                max_tokens=self.config.max_tokens # ë‹µë³€ ê¸¸ì´ ì œí•œ
            ),
            # ì²´ì¸ ìœ í˜• ì„¤ì •
            chain_type="stuff", # ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•œ ë²ˆì— LLMì— ì „ë‹¬
            # ê²€ìƒ‰ê¸° ì—°ê²°
            retriever=retriever,
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—°ê²°
            chain_type_kwargs={"prompt": PROMPT},
            # ì†ŒìŠ¤ ë¬¸ì„œë„ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •
            return_source_documents=True
        )
        logging.info("QA ì²´ì¸ ì„¤ì • ì™„ë£Œ")

    """
        ì‚¬ìš©ìž ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ ë‹µë³€ ìƒì„±
        
        ë‹¤ìŒ ê³¼ì • ìˆ˜í–‰:
        1. ì§ˆë¬¸ì„ ë²¡í„°í™”
        2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        3. ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ LLMì— ì „ë‹¬
        4. ë‹µë³€ ìƒì„±
        5. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    """
    def process_query(self, query: str) -> Tuple[str, List[Document], Dict[str, Any]]:
        # QA ì²´ì¸ì´ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if self.qa_chain is None:
            raise RuntimeError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì‹œìž‘
        start_time = time.time()
        
        # QA ì²´ì¸ ì‹¤í–‰
        # ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ ê³¼ì •ì´ ì¼ì–´ë‚œë‹¤:
        # 1. ì§ˆë¬¸ ë²¡í„°í™”
        # 2. ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ ì‚½ìž…
        # 4. LLM API í˜¸ì¶œ
        # 5. ë‹µë³€ ë°˜í™˜
        result = self.qa_chain({"query": query})

        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ì™„ë£Œ
        end_time = time.time()

        # ê²°ê³¼ íŒŒì‹±
        answer = result["result"] # LLMì´ ìƒì„±í•œ ë‹µë³€
        source_docs = result.get("source_documents", []) # ì°¸ì¡°í•œ ë¬¸ì„œë“¤

        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° (ë¹„ìš© ì¶”ì ìš©)
        total_tokens = len(self.encoding.encode(query + answer)) 
        self.total_tokens_used += total_tokens 

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "processing_time": end_time - start_time, # ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
            "total_tokens": total_tokens, # ì´ë²ˆ ì§ˆë¬¸ì˜ í† í° ìˆ˜
            "sources_count": len(source_docs) # ì°¸ì¡° ë¬¸ì„œ ê°œìˆ˜
        }

        return answer, source_docs, metadata


    """
        ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ìž¥
        
        ê°ì‚¬ ì¶”ì (Audit Trail)ê³¼ ì‹œìŠ¤í…œ ë¶„ì„ì„ ìœ„í•´
        ëª¨ë“  ì§ˆë¬¸-ë‹µë³€ ê¸°ë¡ì„ ì˜êµ¬ ì €ìž¥
    """
    def save_conversation_history(self, query: str, answer: str, sources: List[Document], metadata: Dict[str, Any]) -> None:
        # ì €ìž¥í•  ëŒ€í™” ë¡œê·¸ ìƒì„±
        conversation_log = {
            "timestamp": datetime.now().isoformat(), # íƒ€ìž„ìŠ¤íƒ¬í”„
            "query": query, # ì§ˆë¬¸
            "answer": answer, # ë‹µë³€
            "metadata": metadata, # ì„±ëŠ¥ ë©”íƒ€ë°ì´í„°
             # ì°¸ì¡° ë¬¸ì„œ ì •ë³´ (ë””ë²„ê¹…ìš©)
            "sources": [
                {
                    "content": doc.page_content, # ë¬¸ì„œ ë‚´ìš©
                    "metadata": doc.metadata, # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
                } 
                for doc in sources
            ]
        }
        
        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        log_file = "conversation_history.json"

        # ê¸°ì¡´ ížˆìŠ¤í† ë¦¬ ë¡œë“œ (íŒŒì¼ì´ ìžˆë‹¤ë©´)
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except (json.JSONDecodeError, IOError):
                # íŒŒì¼ì´ ì†ìƒëœ ê²½ìš° ìƒˆë¡œ ì‹œìž‘
                history = []
        
        # ìƒˆ ë¡œê·¸ ì¶”ê°€
        history.append(conversation_log)
        
        # íŒŒì¼ì— ì €ìž¥
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)



    """
        ì‹œìŠ¤í…œ í†µê³„ ì •ë³´ ë°˜í™˜
        
        í˜„ìž¬ ì‹œìŠ¤í…œ ìƒíƒœì™€ ì‚¬ìš© í†µê³„ ì œê³µ
        ëª¨ë‹ˆí„°ë§ê³¼ ë¹„ìš© ì¶”ì ì— í™œìš©
    """
    def get_system_stats(self) -> Dict[str, Any]:
        return {
            "total_documents": len(self.documents), # ë¡œë“œëœ ì´ ë¬¸ì„œ ìˆ˜
            "total_tokens_used": self.total_tokens_used, # ëˆ„ì  í† í° ì‚¬ìš©ëŸ‰
            "estimated_cost": self.total_tokens_used * 0.00001,  # ì¶”ì • ë¹„ìš© (ì˜ˆì‹œ ë‹¨ê°€)
            "vectorstore_loaded": self.vectorstore is not None, # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ìƒíƒœ
            "qa_chain_ready": self.qa_chain is not None # QA ì²´ì¸ ì¤€ë¹„ ìƒíƒœ
        }


    """
        ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œ
        
        ì´ë¯¸ ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ì—¬
        ìž„ë² ë”© ìž¬ìƒì„± ë¹„ìš©ê³¼ ì‹œê°„ ì ˆì•½
    """
    def load_saved_vectorstore(self, vectorstore_path: str = "vectorstore") -> bool:
        try:
            # ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
            if os.path.exists(vectorstore_path):
                # ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œì— í•„ìš”)
                self.embeddings = OpenAIEmbeddings(model=self.config.embedding_model)
                
                # FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
                # ì´ ê³¼ì •ì—ì„œ ì¸ë±ìŠ¤ íŒŒì¼ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ ë¡œë“œ
                self.vectorstore = FAISS.load_local(
                    vectorstore_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True  # ë³´ì•ˆ ê²½ê³  ë¬´ì‹œ
                )
                logging.info("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logging.info("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logging.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    """
        í˜„ìž¬ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ìž¥
        
        ë‚˜ì¤‘ì— ìž¬ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì˜êµ¬ ì €ìž¥
        ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì˜ ê²½ìš° ìž„ë² ë”© ìž¬ìƒì„± ë¹„ìš©ì„ í¬ê²Œ ì ˆì•½
    """
    def save_vectorstore(self, vectorstore_path: str = "vectorstore") -> bool:
        # ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
        try:
            if self.vectorstore is not None:
                # FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ì— ì €ìž¥ (ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ê°€ ë³„ë„ íŒŒì¼ë¡œ ì €ìž¥ëœë‹¤)
                self.vectorstore.save_local(vectorstore_path)
                logging.info(f"ë²¡í„°ìŠ¤í† ì–´ë¥¼ {vectorstore_path}ì— ì €ìž¥í–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                logging.warning("ì €ìž¥í•  ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logging.error(f"ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False


    """
        ìƒ˜í”Œ ì„¤ì • íŒŒì¼ì„ ìƒì„±

        ê¸°ë³¸ ì„¤ì •ê°’ë“¤ì„ í¬í•¨í•œ config.json íŒŒì¼ ìƒì„±
        ì‚¬ìš©ìžê°€ ì´ íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ì‹œìŠ¤í…œì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìžˆë‹¤
    """
    def create_sample_config() -> None:
        config = {
            "chunk_size": 1000, # ì²­í‚¹ í¬ê¸°
            "chunk_overlap": 100, # ì²­í¬ ê²¹ì¹¨
            "max_tokens": 150, # LLM ìµœëŒ€ í† í°
            "model_name": "gpt-4o", # LLM ëª¨ë¸ëª…
            "temperature": 0.4, # LLM ì˜¨ë„ ì„¤ì •
            "retrieval_k": 4, # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "embedding_model": "text-embedding-ada-002", # ìž„ë² ë”© ëª¨ë¸
            "max_retries": 3, # API ìž¬ì‹œë„ íšŸìˆ˜
            "request_timeout": 30 # API íƒ€ìž„ì•„ì›ƒ
        }

        # JSON íŒŒì¼ë¡œ ì €ìž¥
        with open("config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print("ìƒ˜í”Œ ì„¤ì • íŒŒì¼ 'config.json'ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")



    """
        ë©”ì¸ í•¨ìˆ˜ - ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ ì‹¤í–‰

        ì‚¬ìš©ìžì™€ì˜ ìƒí˜¸ìž‘ìš©ì„ í†µí•´ RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³ 
        ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë£¨í”„ë¥¼ ì‹¤í–‰

        ì£¼ìš” ë‹¨ê³„:
        1. ë¡œê¹… ì„¤ì •
        2. ì„¤ì • ë¡œë“œ
        3. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        4. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
        5. QA ì²´ì¸ ì„¤ì •
        6. ëŒ€í™” ë£¨í”„ ì‹¤í–‰
    """
    def main():
        # ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨
            format='%(asctime)s - %(levelname)s - %(message)s', # ë¡œê·¸ í˜•ì‹
            handlers=[
                logging.FileHandler('rag_pipeline.log', encoding='utf-8'), # íŒŒì¼ ë¡œê·¸
                logging.StreamHandler(), # ì½˜ì†” ë¡œê·¸
            ]
        )

        print("=== CSV RAG íŒŒì´í”„ë¼ì¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ===\n")

        try:
            # 1. ì„¤ì • ë¡œë“œ
            config_path = input("ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ìž…ë ¥í•˜ì„¸ìš” (ì—”í„°: ê¸°ë³¸ ì„¤ì • ì‚¬ìš©): ").strip()
            if config_path and Path(config_path).exists():
                config = Config.from_file(config_path) # ì‚¬ìš©ìž ì§€ì • ì„¤ì • íŒŒì¼ ë¡œë“œ
            else:
                config = Config() # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                if not config_path:
                    print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # 2. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
            pipeline = CSVRAGPipeline(config)

            # 3. ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • (ê¸°ì¡´ ê²ƒ ì‚¬ìš© vs ìƒˆë¡œ ìƒì„±)
            use_saved = input("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
            
            if use_saved and pipeline.load_saved_vectorstore():
                # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©
                print("ì €ìž¥ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                pipeline.setup_qa_chain(pipeline.vectorstore)
            else:
                csv_path = "tesla_motors_data.csv"
                print("í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬:", os.getcwd())
                if not os.path.exists(csv_path):
                    raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

                print("\nCSV ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì¤‘...")
                documents = pipeline.load_csv_data(csv_path)

                print("ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
                vectorstore = pipeline.create_vectorstore(documents)

                print("QA ì²´ì¸ ì„¤ì • ì¤‘...")
                pipeline.setup_qa_chain(vectorstore)

                # ë²¡í„°ìŠ¤í† ì–´ ì €ìž¥ ì˜µì…˜
                save_vectorstore = input("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ìž¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
                if save_vectorstore:
                    pipeline.save_vectorstore()

            print("\nâœ… RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            print("ì´ì œ CSV ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
            print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
            print("ì‹œìŠ¤í…œ í†µê³„ë¥¼ ë³´ë ¤ë©´ 'stats'ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.\n")

            # ëŒ€í™” ë£¨í”„
            while True:
                try:
                    query = input("\nì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”: ").strip()

                    if query.lower() in ['quit', 'ì¢…ë£Œ', 'exit']:
                        print("í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•ížˆ ê°€ì„¸ìš”!")
                        break

                    if query.lower() == 'stats':
                        stats = pipeline.get_system_stats()
                        print("\n=== ì‹œìŠ¤í…œ í†µê³„ ===")
                        print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
                        print(f"ì‚¬ìš©ëœ ì´ í† í°: {stats['total_tokens_used']}")
                        print(f"ì¶”ì • ë¹„ìš©: ${stats['estimated_cost']:.4f}")
                        print(f"ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ: {'âœ… ë¡œë“œë¨' if stats['vectorstore_loaded'] else 'âŒ ë¯¸ë¡œë“œ'}")
                        print(f"QA ì²´ì¸ ìƒíƒœ: {'âœ… ì¤€ë¹„ë¨' if stats['qa_chain_ready'] else 'âŒ ë¯¸ì¤€ë¹„'}")
                        continue

                    if not query:
                        print("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue

                    answer, sources, metadata = pipeline.process_query(query)

                    print(f"\nðŸ’¬ ë‹µë³€: {answer}")
                    print(f"\nðŸ“Š ë©”íƒ€ë°ì´í„°:")
                    print(f"  - ì²˜ë¦¬ ì‹œê°„: {metadata['processing_time']:.2f}ì´ˆ")
                    print(f"  - ì‚¬ìš© í† í°: {metadata['total_tokens']}")
                    print(f"  - ì°¸ì¡° ì†ŒìŠ¤: {metadata['sources_count']}ê°œ")

                    if sources:
                        print(f"\nðŸ“š ì°¸ì¡° ì†ŒìŠ¤:")
                        for i, source in enumerate(sources[:3], 1):
                            content_preview = source.page_content[:150].replace('\n', ' ')
                            print(f"  {i}. {content_preview}...")

                    pipeline.save_conversation_history(query, answer, sources, metadata)

                except KeyboardInterrupt:
                    print("\n\ní”„ë¡œê·¸ëž¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                except Exception as e:
                    logging.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    continue

        except Exception as e:
            logging.error(f"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ì˜¤ë¥˜: {e}")
        finally:
            try:
                if 'pipeline' in locals():
                    stats = pipeline.get_system_stats()
                    print(f"\n=== ìµœì¢… í†µê³„ ===")
                    print(f"ì´ ì‚¬ìš© í† í°: {stats['total_tokens_used']}")
                    print(f"ì´ ì¶”ì • ë¹„ìš©: ${stats['estimated_cost']:.4f}")
            except:
                pass


    if __name__ == "__main__":
        if len(sys.argv) > 1 and sys.argv[1] == "--create-config":
            create_sample_config()
        else:
            main()